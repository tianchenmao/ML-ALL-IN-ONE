## 强化学习 Reinforcement Learning

​		强化学习的数学基础和建模工具是**马尔可夫决策过程**（Markov decision process，MDP）。一个 MDP 通常由状态空间$\mathcal{S}$、动作空间$\mathcal{A}$、初始状态分布$P_0$、状态转移函数$\mathcal{\Rho}$、奖励函数$R$、折扣因子$\gamma$等组成。

​		由于某些情况下状态转移函数$\mathcal{\Rho}$和奖励函数$R$未知或难以建模，我们希望智能体通过与环境的交互观察当前状态$s_t$、采取行动$a_t$、接收奖励$r_t$、以及观察下一个状态$s_{t+1}$。然后，智能体使用这些信息来更新其策略。

* 在强化学习中执行策略的主体叫**智能体**（agent）

* 与智能体交互的对象被称作**环境**（environment）

* 每个时刻环境都有一个**状态**$s$（state）

  * **状态空间**$S$表示所有可能存在的状态的合集

* 智能体根据当前状态做出**动作**$a$（action）

  * **动作空间**$A$表示所有可能的动作的合集

* 智能体针对不同状态选择不同动作的概率叫**策略**$\pi$（概率密度函数）

  ​	如状态$s$时，离散动作$a_1,a_2,a_3$，有$\pi(a_1\mid s)=0.3,\pi(a_2\mid s)=0.2,\pi(a_3\mid s)=0.5$

* 环境根据智能体执行的动作$a_t$，状态根据状态转移函数$\Rho$由$s_t$转移为$s_{t+1}$，环境根据奖励函数$R$给出**即时奖励**$r_t$

* 假设环境具有马尔可夫性质（状态转移仅与当前时刻有关），且智能体能够从交互中收集行动**轨迹**$\tau$：$\{s_1,a_1,r_1,s_2,a_2,r_2,\cdots,s_n,a_n,r_n\}$

* 我们不光希望获得最大的即时奖励，更希望在将来整个轨迹中获得最大的累积奖励，叫做**回报**，$\sum_{t=1}^{n}r_t$，考虑到将来的奖励重要性，定义**折扣回报**，$U_t=\sum_{t=1}^{n}\gamma^{t-1}r_t$，其中折扣因子$0<\gamma\le1$。

  * $t$时刻折扣回报的随机性来自$t$时刻后马尔科夫链的状态转移$\Rho:\mathcal{S\times A\rightarrow S}$和策略选择动作$\pi:\mathcal{S\rightarrow A}$，因此**在$t$时刻折扣回报是未知的**。
  * 采样一个真实的轨迹$\{s_t,a_t,r_t,s_{t+1},a_{t+1},r_{t+1},\cdots,s_n,a_n,r_n,\cdots\}$，到达回合结束条件或者折扣项趋于0时，认为轨迹结束。采样一条轨迹得到的折扣回报值是**折扣回报期望的无偏估计**，但有很大方差。

* 既然$t$时刻折扣回报是未知的，我们可以定义**值函数**来估计折扣回报的期望：

  * 动作-状态值函数（值函数）$Q_\pi(s,a)=\mathbb{E}_{\Rho,\pi}[U_t\mid s_t=s,a_t=a]$
    * Q值评价当前给定$s$，采用$a$的好坏。
  * 状态值函数$V_\pi(s)=\mathbb{E}_{A\sim\pi(\cdot\mid s)}[Q_\pi(s,A)]$
    * V值评价当前状态$s$的好坏。

​		强化学习的目标就是**给定当前状态**通过**执行策略给出的动作**以**最大化折扣回报**。强化学习的思想就是利用与环境交互的经验来更新智能体的策略，以试错的态度去处理交互信息，更新策略。

<img src="C:\Users\金刀利刃\Desktop\intro.png" alt="intro" style="zoom:50%;" />

目前强化学习中**无模型的两类方法**：

**值函数方法：**

值函数的方法通过估计值函数（折扣回报的期望），**根据值函数选择动作**，作为智能体的策略。

​		例如今天状态是雨天（$state$），出门选择带伞或不带伞是动作（$action$），对应的值函数（$Q$）相当于今天一整天的心情，假设$Q(s_{雨天},a_{带伞})=1，Q(s_{雨天},a_{不带伞})=-1$，带伞的动作会带来更好的心情，那我们要选择的动作就是带伞：$action=a_{带伞}$。

- 优势：
  - **更为稳定**：值函数方法通常更容易收敛，尤其是在处理高维、连续动作空间或者具有大量状态的环境时。
  - **适用于离散和连续动作空间**：值函数方法在处理离散和连续动作空间（DDPG）时都表现良好。
- 劣势：
  - **采样效率低**：对于某些问题，值函数方法可能需要更多的样本来学习。

**策略优化方法：**

智能体的策略映射当前状态到相应的动作，即给定状态时，**策略给出不同动作的概率**。在策略优化方法中，我们直接对原有的策略进行更新，以最大化长期奖励。

例如策略表示为 $\pi(带伞\mid雨天)=0.5$ ， $\pi(不带伞\mid雨天)=0.5$，然后通过收集奖励的反馈（带伞心情好），我们可以更新策略，使得在雨天时选择带伞的概率增加： $\pi(带伞\mid雨天)=0.9$ ， $\pi(不带伞\mid雨天)=0.1$。

- 优势：
  - **采样效率高**：相对于值函数方法，策略优化方法通常对样本利用更为高效。
  - **直接优化策略**：能够直接对策略进行优化，适用于连续动作空间。
- 劣势：
  - **收敛性较难保证**：由于直接优化策略可能导致非凸优化问题，算法的收敛性相对更难保证。
  - **可能不稳定**：有时策略优化方法在训练过程中可能会出现不稳定的情况。

两类方法的**更新方式**也不同：值函数方法通过优化值函数来指导决策，而策略优化方法则直接对策略进行调整。

**值函数方法：**

* MC迭代更新公式（V值向终点回溯来的$U$值靠近）：
  $$
  V(S_t)\leftarrow V(S_t)+\alpha [U_t-V(S_t)]
  $$

* TD迭代更新公式（以下一步的$R+V$值作为“真实”值，用更可靠的估计取代现有估计）：
  $$
  V(S_t)\leftarrow V(S_t)+\alpha [R_{t}+\gamma V(S_{t+1})-V(S_t)]
  $$
  V值用于评价状态好坏，Q值评价动作的好坏。我们在状态$s$时使用某一个动作$a$的Q值可以一定程度上代替V值。这样就得到SARSA方法。

* SARSA：SARSA是离散动作下的值函数方法。其TD更新规则为：
  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left(r + \gamma Q(s^\prime, a^\prime) - Q(s, a)\right)
  $$

* **Q-Learning：** Q-Learning也是**离散动作**下的值函数方法。其TD更新规则为：
  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left(r + \gamma \max_{a^\prime}Q(s^\prime, a^\prime) - Q(s, a)\right)
  $$
  这个规则表达了在状态$s$采取动作$a$后，根据获得的奖励$r$和下一步状态$s^\prime$的最大动作值$Q(s^\prime,a^\prime)$，更新当前动作值$Q(s, a)$的过程。

  由于采样的$\{s,a,r,s^\prime\}$与状态转移$\Rho$和奖励函数$R$有关，与策略$\pi$无关，在更新策略后仍可以继续利用该样本。我们可以将采样的四元组保存在**经验回放池**（**Experience Replay Buffer**）中。

* **DQN（Deep Q-Network）：**

  <img src="C:\Users\金刀利刃\Desktop\DQN.png" alt="DQN" style="zoom:35%;" />

   DQN是**离散动作**下基于神经网络的值函数方法，使用深度学习逼近 Q 函数。Q网络的参数为$\theta$，时序差分仍然为：
  $$
  Q(s,a;\theta) \leftarrow Q(s,a;\theta) + \alpha \left(r + \gamma \max_{a'}Q(s', a';\theta) - Q(s, a;\theta)\right)
  $$
  其中$r + \gamma \max_{a'}Q(s', a';\theta) - Q(s, a;\theta)$ 叫做TD error（TD误差）。

  定义其损失函数为TD误差平方的期望：
  $$
  L(\theta) = \mathbb{E}\left[\left(r + \gamma \max_{a'}Q(s', a'; \theta) - Q(s, a; \theta)\right)^2\right]
  $$
  最小化损失函数就是使**估计值Q**接近"有一个采样的"更准确的估计值**目标Q**。以一定步长最小化损失函数与时序差分迭代更新是完全一致的。且DQN也可以使用经验回放，每次可以抽样一批数据同时计算。

  DQN的问题——**高估问题**

  由于估计是自举的加上每次迭代采用最大动作值，带来了**高估问题**

  ……to be done……

  如使用目标Q网络：

  $L(\theta) = \mathbb{E}\left[\left(r + \gamma \max_{a'}Q(s', a'; \theta^-) - Q(s, a; \theta)\right)^2\right]$
  其中，$\theta$ 是当前 Q 网络的参数，$\theta^-$是目标 Q 网络的参数，通过固定周期或滑动平均方式进行更新。

* DDPG（确定性策略梯度）见下面

* MADDPG（多智能体确定性策略梯度）见下面

**策略优化方法：**

- **Policy Gradient：** Policy Gradient方法直接优化策略。策略函数或策略网络根据当前状态$s$输出动作$a$的概率分布，其更新规则为：
  $\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$
  其中，$\theta$是策略参数，$J(\theta)$是期望回报的函数。更新时通过梯度上升的方式使期望回报最大化。
$$
  \nabla_\theta J(\theta) = E_S[E_{A\simπ(\cdot|S,θ)}[\frac {∂\logπ(A|S;θ)}{∂θ}Q_π(S,A)]]
$$
  其中估计Q值使用蒙特卡洛采样的方法叫REINFORCE，使用一个网络近似则是Actor-Critic方法。

- **Actor-Critic：** Actor-Critic方法结合了值函数和策略网络。Actor负责执行动作，Critic评估执行动作的价值。更新规则为：
  $\theta_{\text{actor}} \leftarrow \theta_{\text{actor}} + \alpha \nabla_\theta \log \pi(a|s; \theta_{\text{actor}})Q(s, a; \theta_{\text{critic}})$
  $\theta_{\text{critic}} \leftarrow \theta_{\text{critic}} + \beta(r + \gamma Q(s', a'; \theta_{\text{critic}}) - Q(s, a; \theta_{\text{critic}}))$



### DDPG (Deep Deterministic Policy Gradients):

1. **任务类型：** DDPG 主要用于解决连续动作空间的问题，Actor网络根据当前状态$s$给定确定的动作$a$，这意味着智能体可以在一个连续的动作范围内选择动作，而不是像DQN一样在离散的动作集中选择。

2. **架构：** 

   DDPG 结合了 DQN（Deep Q-Network）和确定性策略梯度（DPG）。它包含一个用于估计动作值函数的评论网络（Critic Network）和一个用于估计策略的行动者网络（Actor Network）。Actor 利用策略梯度更新，Critic利用最小化均方差损失更新。

   DDPG 使用策略梯度方法，其中行动者网络学习输出给定状态的确定性动作，而评论网络则学习评估这个动作的价值。

3. **经验回放：** DDPG 也使用经验回放来训练网络，通过从经验池中随机抽样来进行训练，以提高样本的效率和训练的稳定性。

4. **目标网络：** 为了提高训练的稳定性，DDPG 使用两套网络：Actor-Ctitic网络和目标Actor-Critic网络，这两个目标网络的参数更新是通过软更新进行的。

<img src="C:\Users\金刀利刃\Desktop\DDPG.png" alt="intro" style="zoom:50%;" />

### MADDPG (Multi-Agent Deep Deterministic Policy Gradients):

1. **多智能体环境：** MADDPG 被设计用于多智能体环境，其中有多个智能体相互作用和影响彼此的决策。

2. **独立智能体：** MADDPG 将每个智能体视为一个独立的学习实体，但它同时考虑了其他智能体的行为对当前智能体的影响。

3. **共同训练：** MADDPG 使用集体经验回放来协同训练多个智能体。每个智能体的策略梯度更新都取决于整个团队的奖励信号，而不仅仅是个体的奖励。

4. **局部观察：** 每个智能体通常只能观察到局部信息，即只能看到与其相关的部分环境状态，这使得智能体需要通过合作和竞争来实现全局目标。

5. **协同学习：** MADDPG 旨在通过协同学习实现更好的全局性能，智能体的策略被训练以平衡个体和团队的利益。

DDPG 和 MADDPG 分别适用于解决连续动作空间和多智能体环境中的问题。 MADDPG 在处理多智能体协同学习问题上具有一定的优势。

<img src="C:\Users\金刀利刃\Desktop\MADDPG.png" alt="intro" style="zoom:50%;" />

MADDPG的创新点：集中训练，分散执行。

Actor 利用策略梯度更新，每个Actor只关心自己的状态与动作。策略梯度：

<img src="C:\Users\金刀利刃\AppData\Roaming\Typora\typora-user-images\image-20231202140644276.png" alt="image-20231202140644276" style="zoom:60%;" />

Critic利用最小化均方差损失更新，每个Critic都需要所有智能体的观测和动作。均方差损失：

<img src="C:\Users\金刀利刃\AppData\Roaming\Typora\typora-user-images\image-20231202140649580.png" alt="i mage-20231202140649580" style="zoom:60%;" />

由于每个智能体的奖励函数是可以自由定义的，MADDPG可以用于多种任务，完全合作，竞争合作，完全竞争。

在应用于多架无人机协同任务时，可以设置每个无人机的观测值包括{位置，速度，加速度}，动作值可以设置为{三个方向的加速度}或者{三个方向的速度}等，所有无人机共用一个奖励来完成协同任务。

难题：采样效率低，如何利用基于模型的强化学习